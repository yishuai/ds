class: middle, center

# 文件整理

---
# 内容
- 数据源示例
- 文件格式
- 文件编码
- 文件大小
- Shell 和命令行工具
- 表形状和粒度

---
# 实例
- 急症数据
  - 文本
- 餐馆安全
  - 卫生检查记录，分数，违规描述

---
# 分隔格式

- 第一行：列名
- 分隔符
  - 逗号分隔：comma-separated-values, or CSV
  - Tab分隔：tab-separated values or TSV
  - 空格
  - 分号
- 每一行
  - 新行 \n, \r\n

---
# Python pathlib 库

用 path 函数 创建 路径全称

      from pathlib import Path
      insp_path = Path() / 'data' / 'inspections.csv'
注意：
  
    Windows path 是 左斜杠：C:\files\data.csv
  
    OS 或 Linux 或 Unix 是右斜杠：~/files/data.csv

---
# 读文件

      text = insp_path.read_text()
打印前5行

      print('\n'.join(text.split('\n')[:5]))

      "business_id","score","date","type"
      19,"94","20160513","routine"
      19,"94","20171211","routine"
      24,"98","20171101","routine"
      24,"98","20161005","routine"

---
# 日期格式转换

- Score 和 date 是 string
- 需要转换 为 number 和 date 格式，方便后续处理

---
# key - value 格式
- JSON
- XML
- HTML
- 可以 嵌套

---
# 日志

松散格式，可根据观察到的模式，提取想要的信息

网站服务器日志

      169.237.46.168 - - [26/Jan/2004:10:47:58 -0800]
      "GET /stat141/Winter04 HTTP/1.1" 301 328 
      "http://anson.ucdavis.edu/courses" 
      "Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.0; 
      .NET CLR 1.1.4322)"
无线设备日志

      t=1139644637174;id=00:02:2D:21:0F:33;
      pos=2.0,0.0,0.0;degree=45.5; 
      00:14:bf:b1:97:8a=-33,2437000000,3;

---
# 文件编码
- ASCII
  - 100 001 代表字母：A
- Latin-1 (ISO-8859-1)
- UTF-8
  - 超过 1M 字符， 后向兼容 ASCII

---
# 推断文件编码
chardet 库 函数 detect 可以推断文件编码

信心 分数 0 - 1 之间

      import chardet

      line = '{:<25} {:<10} {}'.format

      print(line('File Name', 'Encoding', 'Confidence'))

      for filepath in Path('data').glob('*'):
          result = chardet.detect(filepath.read_bytes())
          print(line(str(filepath), result['encoding'], 
              result['confidence']))
猜总有不确定性。最好根据文档说明来设置。

---
# 文件编码

如果是 Latin-1 (ISO-8859-1)， 但不告诉 pandas，就直接读入，会出错

      >>> pd.read_csv('data/businesses.csv')

      [...stack trace omitted...]
      UnicodeDecodeError: 'utf-8' codec can not decode byte 
      0xd1 in position 8: invalid continuation byte

      bus = pd.read_csv('data/businesses.csv', 
          encoding='ISO-8859-1')

---
# 文件大小

      os.path.getsize(file)
- 如果文件几百兆，那么 用 Python 把它全部读入变量中，就会让电脑变慢。
  - 用 pandas 读入一个文件，一般会花掉 5 倍的内存
- 如果有 1 GiB 可用 RAM，pandas 不太可能读取 1 GiB 文件。
  - 内存由计算机上运行的所有程序共享，包括操作系统、Web 浏览器、其它应用程序。总 RAM 为 4 GiB 的计算机在运行许多应用程序时可能只有 1 GiB 可用 RAM。

---
# 大文件的读入方法
- 只读入需要的“列”
- 随机采样
- 读一部分，比如“一天”
- 用 SQL 数据库 存数据
  - MySQL，PostgreSQL
- 大数据方法
  - MapReduce、Spark、Ray

---
# Shell 和 命令行工具

语法：command -options arg1 arg2

例 

      ls -l -h figures/
      ls -lh figures/
- options，可以组合
   -  -l 提供有关每个文件的额外信息
   -  -h 以更易于理解的格式提供文件大小
- Jupyter里，用 ！（感叹号）运行 Shell 命令

---
# 查看文件大小

      wc data/DAWN-Data.txt

      229211 22695570 280095842 data/DAWN-Data.txt
      行数 单词数 字符数

---
# 查看磁盘使用

disk usage

      du -Lsh data/*

      267M	data/DAWN-Data.txt
      648K	data/businesses.csv
-s 显示文件和文件夹大小

-h 以标准 KiB、MiB 或 GiB 格式显示大小

---
# head

显示文件开头的几行

    head -4 data/inspections.csv
    head -n 4 data/inspections.csv
默认显示 10 行

---
# cat 

打印整个文件

大文件会导致系统崩溃哦

最好用 head 或者 tail

---
# file 得到文件编码

      file -I data/*

      data/DAWN-Data.txt:   text/plain; charset=us-ascii
      data/businesses.csv:  application/csv; 
                                charset=iso-8859-1

---
# 表的 shape 和 granularity
- shape：行数，列数
- granularity：一行的内容

---
# 餐馆卫生检查案例

三个表
- 餐馆表
- 检查表
- 问题表

---
# 餐馆表

- 问题
  - id 是 唯一的吗？
- 方法
  - 比较一下“行数”和唯一的值的数目

            print("Number of records:", len(bus))

            print("Number of unique business ids:", 
              len(bus['business_id'].unique()))

---
# 检查表

- 问题
  - 一个餐馆，一天检查几次？
- 方法
  - 用 餐馆id 和 date 做 group，然后 size 统计次数

            (insp
            .groupby(['business_id', 'date'])
            .size()
            .sort_values(ascending=False)
            .head(5)
            )

---
# 餐馆一天检查次数

        business_id  date    
        64859        20150924    2
        87440        20160801    2
        77427        20170706    2
        19           20160513    1
        71416        20171213    1
        dtype: int64
- 只有三个餐馆同一天检查了2次
  - 数据质量问题？还是确实检查了2次
  - 暂时去掉这3对记录
- 可以用 餐馆 + Date 作为分析单元：Primary Key

---
# 问题表
- 每次检查，会有多个问题，每个问题一行
- 其中还包括了检查发现的问题的纠正日期

        viol.loc[39039, 'description']

        'High risk vermin infestation  
            [ date violation corrected: 12/15/2017 ]'

---
# 固定位置文件

如果一行里太多列，加“逗号”的话，太多，就可用固定位置方式存储

      dawn_path = Path() / 'data' / 'DAWN-Data.txt'
      head(dawn_path, width=55)

      1 2251082    .9426354082   3 4 1 2201141 2 865
      2 2291292   5.9920106887   911 1 3201134 12077
      3 7 7 251   4.7231718669   611 2 2201143 12313
      410 8 292   4.0801470012   6 2 1 3201122 1 234
      5 122 942   5.1777093467  10 6 1 3201134 3 865

???

      def head(filepath, n=5, width=-1):
          '''打印文件前 n 行的width个字符'''
          with filepath.open() as f:
              for _ in range(n):
                  (print(f.readline(), end='') if width < 0  
                  else print(f.readline()[:width]))

---
# 读取固定位置文件

用 pandas.read_fwf 读

指定位置

      colspecs = [(0,6), (14,29), (33,35), (35, 37), 
            (37, 39), (1213, 1214)]
      varNames = ["id", "wt", "age", "sex", "race","type"]
      dawn = pd.read_fwf('data/DAWN-Data.txt', 
            colspecs=colspecs, header=None, index_col=0, 
            names=varNames)

---
# 目标对象权重调整

- 急诊室数据案例，样本有一个 Weight 特征

  - 这个权重 表示 像这样的急诊就诊出现在样本中的可能性。即具有相似特征的访问，例如访问者年龄、种族、访问地点和一天中的时间。

  - 数据通过它，用这个数据反映一年内就诊的人群。
  - 当我们汇总统计、构建直方图和拟合模型时，必须将权重应用于每条记录。

  - 例：假设您进行了一项调查，45% 的受访者年龄在 18 岁以下，但根据人口普查局的数据，只有 22% 的人口年龄在 18 岁以下。您可以调整您的调查答复以反映人口，对 18 岁以下的人使用较小的权重 (22/45)，对 18 岁及以上的人使用较大的权重 (78/55)。

---
# 例

- 调查结果
  - 统计 100 人
  - 18岁以下 45人，其中 44 人用手机
  - 以上 55人，其中 35 人用手机
- 人口普查局数据
  - 22% 的人口年龄在 18 岁以下
- 人口中，一百人中，使用手机的人数

        (22/45) \times 44 + (78/55) \times 35 = 71

---
# 例

numpy.average 可以考虑 权重

考虑 和 不考虑权重 时 就诊女性比例计算结果：

      print(f'Unweighted percent female: 
        {np.average(dawn["sex"] == 2):.1%}')
      
      print(f'  Weighted percent female:',
        f'{np.average(dawn["sex"] == 2, 
            weights=dawn["wt"]):.1%}')

      Unweighted percent female: 48.0%
        Weighted percent female: 52.3%

---
# 总结
- 文件整理很重要，发现 会影响 后续 分析 的 问题
- 读文件
  - 格式、编码
  - 文件大小、工作方法
- 命令行工具
- 评估 数据 shape 和 粒度（granularity）
  - 一行里面有什么
  - 有没有混合的，比如：总结行
  - 要不要 合并
  - 权重

---
# 练习

textbook-mastercontent/ch/08

1-files_formats.ipynb
2-files_encoding.ipynb
3-files_size.ipynb
4-files_command_line.ipynb
5-files_granularity.ipynb

